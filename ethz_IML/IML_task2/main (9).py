# -*- coding: utf-8 -*-
"""EDA_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/angelgldh/ethz_school_work/blob/main/ethz_IML/IML_task2/EDA_notebook.ipynb
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import boxcox, yeojohnson
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import DotProduct, RBF, Matern, RationalQuadratic
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import r2_score

def impute_avg_per_season(df, season_col="season"):
    return df.groupby(season_col).transform(lambda x: x.fillna(x.median())).join(df[season_col])

def linear_regression_imputation(df, features_to_impute):
    # Create a copy of the data frame to avoid modifying the original data frame
    dfc = df.copy()

    # One-hot encode categorical variables
    dfc = pd.get_dummies(dfc, columns=['season'], prefix=['season'])

    for feature in features_to_impute:
        # Filter data with and without missing values
        data_with_feature = dfc[dfc[feature].notnull()]
        data_missing_feature = dfc[dfc[feature].isnull()]

        # Create X and y for the feature
        X = data_with_feature.drop(features_to_impute, axis='columns').values
        y = data_with_feature[feature].values

        # Train and predict using KFold cross-validation
        np.random.seed(42)
        kf = KFold(n_splits=4)
        for train_index, test_index in kf.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            clf = LinearRegression()
            clf.fit(X_train, y_train)
            y_test_pred = clf.predict(X_test)

        # Impute missing values
        X_missing = data_missing_feature.drop(features_to_impute, axis='columns').values
        y_missing_pred = clf.predict(X_missing)
        dfc.loc[dfc[feature].isnull(), feature] = y_missing_pred

    return dfc

def data_loading():
    """
    This function loads the training and test data, preprocesses it, removes the NaN values and interpolates the missing 
    data using imputation

    Parameters
    ----------
    Returns
    ----------
    X_train: matrix of floats, training input with features
    y_train: array of floats, training output with labels
    X_test: matrix of floats: dim = (100, ?), test input with features
    """
    # Load training data
    train_df = pd.read_csv("train.csv")
    
    print("Training data:")
    print("Shape:", train_df.shape)
    print(train_df.head(2))
    print('\n')
    
    # Load test data
    test_df = pd.read_csv("test.csv")

    print("Test data:")
    print(test_df.shape)
    print(test_df.head(2))

    # Drop missing values of price_CHF
    train_df = train_df.dropna(subset = 'price_CHF')

    # Dummy initialization of the X_train, X_test and y_train   
    X_train = np.zeros_like(train_df.drop(['price_CHF'],axis=1))
    y_train = np.zeros_like(train_df['price_CHF'])
    X_test = np.zeros_like(test_df)

    # 1. Missing values
    train_df_avg_imputed = impute_avg_per_season(train_df)
    test_df_avg_imputed = impute_avg_per_season(test_df)

    # 2. Yeo Johnson transformation
    train_df_yj = train_df_avg_imputed.copy() ;  test_df_yj = test_df_avg_imputed.copy()
    feature_names = [col for col in train_df_boxcox.columns if col not in ['season', 'price_CHF']]
    for feature in feature_names:
      train_df_yj[feature], _ = yeojohnson(train_df_yj[feature])
      test_df_yj[feature], _ = yeojohnson(test_df_yj[feature])

    # 3. Scale the data
    train_df_normalized = train_df_yj.copy()
    train_df_normalized [feature_names] = scaler.fit_transform(train_df_yj[[f'{feature}' for feature in feature_names]])

    test_df_normalized = test_df_yj.copy()
    test_df_normalized[feature_names] = scaler.fit_transform(test_df_yj[[f'{feature}' for feature in feature_names]])


    X_train = train_df_normalized.drop(['price_CHF'],axis=1)
    y_train = train_df_normalized['price_CHF']
    X_test = test_df_normalized.copy()

    assert (X_train.shape[1] == X_test.shape[1]) and (X_train.shape[0] == y_train.shape[0]) and (X_test.shape[0] == 100), "Invalid data shape"
    return X_train, y_train, X_test

def modeling_and_prediction(X_train, y_train, X_test):
    """
    This function defines the model, fits training data and then does the prediction with the test data 

    Parameters
    ----------
    X_train: matrix of floats, training input with 10 features
    y_train: array of floats, training output
    X_test: matrix of floats: dim = (100, ?), test input with 10 features

    Returns
    ----------
    y_test: array of floats: dim = (100,), predictions on test set
    """

    y_pred=np.zeros(X_test.shape[0])

    ## 1. Modelling

    all_train_df = pd.concat([X_train, y_train], axis=1)

    # Group the dataset by the 'season' column
    grouped = all_train_df.groupby('season')

    # Create a dictionary to store the best models for each season
    best_models = {}

    # Iterate over each season group
    for season, group in grouped:
        print(f"Fitting Gaussian Process model for season {season}:")
        
        X_group = group.drop(["price_CHF", "season"], axis=1)
        y_group = group["price_CHF"]
        
        # Split the data into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X_group, y_group, test_size=0.2, random_state=42)
        
        # Define kernels to test
        kernels = [
            ("DotProduct", DotProduct()),
            ("RBF", RBF()),
            ("Matern", Matern()),
            ("RationalQuadratic", RationalQuadratic()),
        ]
        
        best_score = float("-inf")
        best_kernel = None
        best_model = None
        
        # Iterate over kernels and fit the model using cross-validation
        for kernel_name, kernel in kernels:
            gpr = GaussianProcessRegressor(kernel=kernel)
            scores = cross_val_score(gpr, X_train, y_train, cv=10, scoring='neg_mean_squared_error')
            mean_score = scores.mean()
            
            print(f"Kernel: {kernel_name}, Mean Cross-Validation MSE: {mean_score}")

            if mean_score > best_score:
                best_score = mean_score
                best_kernel = kernel_name
                best_model = gpr
        
        # Fit the best model on the whole training data
        best_model.fit(X_train, y_train)
        print(f"Best kernel for season {season}: {best_kernel}")
        
        # Store the best model for the current season in the best_models dictionary
        best_models[season] = best_model
        
        # Evaluate the model on the test set
        test_score = best_model.score(X_test, y_test)
        print(f"Test set R^2 score for season {season}: {test_score}\n")


    ## 2. Prediction 

    X_test['price_CHF'] = np.nan
    for season, model in best_models.items():
        season_test_data = X_test[X_test['season'] == season]
        X_season_test = season_test_data.drop(['price_CHF', 'season'], axis=1)
        X_test.loc[season_test_data.index, 'price_CHF'] = model.predict(X_season_test)

    # Reset the index of the test DataFrame
    X_test = X_test.reset_index()

    # Create a new DataFrame with only the index and predicted price_CHF columns
    y_pred = X_test[['price_CHF']]


    assert y_pred.shape == (100,), "Invalid data shape"
    return y_pred

if __name__ == "__main__":
    # Data loading
    X_train, y_train, X_test = data_loading()
    # The function retrieving optimal LR parameters
    y_pred=modeling_and_prediction(X_train, y_train, X_test)
    # Save results in the required format
    dt = pd.DataFrame(y_pred) 
    dt.columns = ['price_CHF']
    dt.to_csv('results.csv', index=False)
    print("\nResults file successfully generated!")