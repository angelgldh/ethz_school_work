{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_cost_gradient(X, y, w, alpha):\n",
    "    n = len(y)\n",
    "    y_pred = X @ w\n",
    "    cost = (1 / (2 * n)) * np.sum((y_pred - y) ** 2) + (alpha / 2) * np.sum(w[1:] ** 2)\n",
    "    # To calculate the gradient, one needs to take the derivative of the cost with respect to the weight vector\n",
    "    gradient = (1 / n) * (X.T @ (y_pred - y)) + alpha * np.hstack(([0], w[1:])) * (1 / n)\n",
    "    return cost, gradient    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, lam):\n",
    "    \"\"\"\n",
    "    This function receives training data points, then fits the ridge regression on this data\n",
    "    with regularization hyperparameter lambda. The weights w of the fitted ridge regression\n",
    "    are returned. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: matrix of floats, dim = (135,13), inputs with 13 features\n",
    "    y: array of floats, dim = (135,), input labels)\n",
    "    lam: float. lambda parameter, used in regularization term\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    w: array of floats: dim = (13,), optimal parameters of ridge regression\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    w = np.zeros((p,))\n",
    "    method = \"analytical\"\n",
    "\n",
    "    if method ==\"gradient_descent\":\n",
    "        # Option 1: gradient descent \n",
    "        # To fit Ridge regression: gradient descent will be applied\n",
    "        learning_rate = 1e-6\n",
    "        max_iter = 2000\n",
    "        for ii in range(max_iter):\n",
    "            cost, gradient = ridge_cost_gradient(X, y, w, alpha = lam)\n",
    "            w -= learning_rate*gradient\n",
    "            # if (ii % 100 == 0):\n",
    "            #    print(f\"Iteration {ii} completed. Cost: {cost}\")\n",
    "    elif method == \"analytical\":\n",
    "        # Option 2: closed-fomr solution\n",
    "        A = np.linalg.inv(np.dot(X.T, X) + lam * np.identity(n=p))\n",
    "        B = X.T @ y\n",
    "        w =  np.dot(A,B)\n",
    "\n",
    "    assert w.shape == (13,)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(w, X, y):\n",
    "    \"\"\"This function takes test data points (X and y), and computes the empirical RMSE of \n",
    "    predicting y from X using a linear model with weights w. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w: array of floats: dim = (13,), optimal parameters of ridge regression \n",
    "    X: matrix of floats, dim = (15,13), inputs with 13 features\n",
    "    y: array of floats, dim = (15,), input labels\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    RMSE: float: dim = 1, RMSE value\n",
    "    \"\"\"\n",
    "    RMSE = 0\n",
    "    n, p = X.shape\n",
    "    \n",
    "    y_pred = X @ w\n",
    "    RMSE = np.sqrt((1/n)*np.sum((y - y_pred)**2) )\n",
    "\n",
    "    assert np.isscalar(RMSE)\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_LR_RMSE(X, y, lambdas, n_folds):\n",
    "    \"\"\"\n",
    "    Main cross-validation loop, implementing 10-fold CV. In every iteration (for every train-test split), the RMSE for every lambda is calculated, \n",
    "    and then averaged over iterations.\n",
    "    \n",
    "    Parameters\n",
    "    ---------- \n",
    "    X: matrix of floats, dim = (150, 13), inputs with 13 features\n",
    "    y: array of floats, dim = (150, ), input labels\n",
    "    lambdas: list of floats, len = 5, values of lambda for which ridge regression is fitted and RMSE estimated\n",
    "    n_folds: int, number of folds (pieces in which we split the dataset), parameter K in KFold CV\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    avg_RMSE: array of floats: dim = (5,), average RMSE value for every lambda\n",
    "    \"\"\"\n",
    "    RMSE_mat = np.zeros((n_folds, len(lambdas)))\n",
    "\n",
    "    kf = KFold(n_splits = n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Evaluate the obtained RMSE per every value of the proposed regularization parameters\n",
    "    for ii in range(len(lambdas)):\n",
    "        lam = lambdas[ii]\n",
    "\n",
    "        RMSE_at_this_lambda = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            w = fit(X_train, y_train, lam)\n",
    "            RMSE_at_this_lambda.append(calculate_RMSE(w, X_test, y_test))\n",
    "            \n",
    "        RMSE_mat[:, ii] = RMSE_at_this_lambda\n",
    "\n",
    "\n",
    "    assert RMSE_mat.shape == (n_folds, len(lambdas))\n",
    "\n",
    "    avg_RMSE = np.mean(RMSE_mat, axis=0)\n",
    "    assert avg_RMSE.shape == (5,)\n",
    "    return avg_RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Data loading\n",
    "    data = pd.read_csv(\"train.csv\")\n",
    "    y = data[\"y\"].to_numpy()\n",
    "    data = data.drop(columns=\"y\")\n",
    "    # print a few data samples\n",
    "    # print(data.head())\n",
    "\n",
    "    X = data.to_numpy()\n",
    "    # The function calculating the average RMSE\n",
    "    lambdas = [0.1, 1, 10, 100, 200]\n",
    "    n_folds = 10\n",
    "    avg_RMSE = average_LR_RMSE(X, y, lambdas, n_folds)\n",
    "    # Save results in the required format\n",
    "    np.savetxt(\"./results.csv\", avg_RMSE, fmt=\"%.12f\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
